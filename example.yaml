apiVersion: v1
kind: Namespace
metadata:
  labels:
    kubernetes.io/metadata.name: gpu-test-workloads
    pod-security.kubernetes.io/enforce: privileged
  name: gpu-test-workloads
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cuda-sample-vector-add
  namespace: gpu-test-workloads
  labels:
    app: cuda-sample-vector-add
spec:
<<<<<<< HEAD
  replicas: 1
  selector:
    matchLabels:
      app: cuda-sample-vector-add
  template:
    metadata:
      labels:
        app: cuda-sample-vector-add
    spec:
      containers:
        - name: cuda-sample-vector-add
          image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04
          command:
            - /bin/bash
            - '-c'
            - '--'
          args:
            - while true; do /cuda-samples/vectorAdd; done
          resources:
            limits:
              nvidia.com/gpu: 1 # declare how many physical GPUs the pod needs
              nvidia.com/gpumem: 3000 # Each vGPU contains 3000M device memory （Optional,Integer）
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      hostPID: true
      securityContext: {}
      schedulerName: default-scheduler
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      priorityClassName: system-cluster-critical
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
=======
  containers:
    - name: ubuntu-container
      image: ubuntu:18.04
      command: ["bash", "-c", "sleep 86400"]
      resources:
        limits:
          nvidia.com/gpu: 2 # requesting 2 vGPUs
          #nvidia.com/gpumem: 3000 # Each vGPU containers 3000M device memory
          nvidia.com/gpumem-percentage: 50 #Each vGPU containers 50% device memory of that GPU. Can not be used with nvidia.com/gpumem
          #nvidia.com/gpucores: 90 # Utilization limit of this vGPU is set to 50% of total GPU utilization 
          #nvidia.com/priority: 0 # We only have two priority class, 0(high) and 1(low), default: 1 
          #The utilization of high priority task won't be limited to resourceCores unless sharing GPU node with other high priority tasks.
          #The utilization of low priority task won't be limited to resourceCores if no other tasks sharing its GPU.
    - name: ubuntu-container0
      image: ubuntu:18.04
      command: ["bash", "-c", "sleep 86400"]
    - name: ubuntu-container1
      image: ubuntu:18.04
      command: ["bash", "-c", "sleep 86400"]
      resources:
        limits:
          nvidia.com/gpu: 2 # requesting 2 vGPUs
          nvidia.com/gpumem: 3000
          #nvidia.com/gpucores: 90

>>>>>>> 32fbedb (update device_plugin version to nvidia v0.14.0)
